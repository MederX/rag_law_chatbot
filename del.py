import os
from loguru import logger
from langchain_community.vectorstores import FAISS
import re

logger.add("log/kyrgyz_laws_rag.log", format="{time} {level} {message}", level="DEBUG", rotation="100 KB", compression="zip")

def get_index_db():
    logger.debug('...get_index_db')
    from langchain_huggingface import HuggingFaceEmbeddings

    model_id = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
    embeddings = HuggingFaceEmbeddings(
        model_name=model_id,
        model_kwargs={'device': 'cuda'},  
    )

    db_file_name = 'db/laws_db'
    file_path = db_file_name + "/index.faiss"
    
    if os.path.exists(file_path):
        logger.debug('–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã')
        db = FAISS.load_local(db_file_name, embeddings, allow_dangerous_deserialization=True)
    else:
        logger.debug('–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ .txt —Ñ–∞–π–ª–æ–≤')
        from langchain_community.document_loaders import TextLoader

        dir = 'laws'
        documents = []
        loaded_files = []
        
        for root, dirs, files in os.walk(dir):
            for file in files:
                if file.endswith(".txt"):
                    try:
                        logger.debug(f'–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file}')
                        file_path = os.path.join(root, file)
                        loader = TextLoader(file_path, encoding='utf-8')
                        file_docs = loader.load()
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∑–∞–∫–æ–Ω–∞
                        for doc in file_docs:
                            doc.metadata['source_file'] = file
                            doc.metadata['law_name'] = file.replace('.txt', '').replace('_', ' ')
                        
                        documents.extend(file_docs)
                        loaded_files.append(file)
                    except Exception as e:
                        logger.error(f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file}: {e}')

        logger.info(f'–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(loaded_files)}')
        logger.info(f'–§–∞–π–ª—ã: {loaded_files}')

        from langchain.text_splitter import RecursiveCharacterTextSplitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  
            chunk_overlap=150,  
            separators=["\n\n", "\n", ". ", " "],  
            keep_separator=True
        )
        source_chunks = text_splitter.split_documents(documents)
        
        logger.info(f'–°–æ–∑–¥–∞–Ω–æ —á—É–Ω–∫–æ–≤: {len(source_chunks)}')

        db = FAISS.from_documents(source_chunks, embeddings)
        db.save_local(db_file_name)

    return db

def get_message_content(topic, db, k):
    logger.debug('...get_message_content')
    
 
    try:
        docs = db.max_marginal_relevance_search(topic, k=k, fetch_k=k*2)
    except:

        docs = db.similarity_search(topic, k=k)
    
 
    sources_content = {}
    for doc in docs:
        source = doc.metadata.get('source_file', 'unknown')
        law_name = doc.metadata.get('law_name', source)
        
        if source not in sources_content:
            sources_content[source] = {
                'law_name': law_name,
                'chunks': []
            }
        sources_content[source]['chunks'].append(doc.page_content)
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    message_content = ""
    for i, (source, data) in enumerate(sources_content.items()):
        message_content += f"\n=== –ó–ê–ö–û–ù: {data['law_name']} ===\n"
        for j, chunk in enumerate(data['chunks']):
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            clean_chunk = re.sub(r'\n+', ' ', chunk.strip())
            clean_chunk = re.sub(r'\s+', ' ', clean_chunk)
            message_content += f"{clean_chunk}\n"
        message_content += "\n"
    
    logger.debug(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources_content)}")
    return message_content.strip()

def get_model_response(topic, message_content):
    logger.debug('...get_model_response')

    from langchain_ollama import ChatOllama
    llm = ChatOllama(
        model="llama3.2:3b-instruct-fp16", 
        temperature=0.1,  
        top_p=0.9,
        repeat_penalty=1.1
    )


    rag_prompt = """–¢—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∑–∞–∫–æ–Ω–∞–º –ö—ã—Ä–≥—ã–∑—Å–∫–æ–π –†–µ—Å–ø—É–±–ª–∏–∫–∏. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ó–ê–ö–û–ù–û–í –ö–†:
{context}

–í–û–ü–†–û–°: {question}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –û—Ç–≤–µ—á–∞–π –°–¢–†–û–ì–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
3. –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏/–ø—É–Ω–∫—Ç—ã –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏ —Å—Å—ã–ª–∫–µ
4. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü—É
5. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫—Ä–∞—Ç–∫–∏–º (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
6. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

–û–¢–í–ï–¢:"""

    from langchain_core.messages import HumanMessage
    prompt = rag_prompt.format(context=message_content, question=topic)
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])

        answer = response.content.strip()

        english_patterns = [
            r'\b(according to|based on|however|therefore|thus|themselves|moreover|furthermore)\b',
            r'\b(Article|Section|Chapter|Paragraph)\b'
        ]
        
        for pattern in english_patterns:
            answer = re.sub(pattern, '', answer, flags=re.IGNORECASE)
        

        answer = re.sub(r'\s+', ' ', answer).strip()
        
        return answer
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."

def validate_answer(answer, context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
    if len(answer.strip()) < 20:
        return False, "–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π"

    english_words = re.findall(r'\b[A-Za-z]+\b', answer)
    if len(english_words) > 1:  # –î–æ–ø—É—Å–∫–∞–µ–º –º–∏–Ω–∏–º—É–º –∞–Ω–≥–ª. —Å–ª–æ–≤
        return False, f"–ù–∞–π–¥–µ–Ω—ã –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞: {english_words}"
    
    return True, "OK"

def interactive_chat():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —á–∞—Ç–∞"""
    print("üèõÔ∏è  –ß–∞—Ç-–±–æ—Ç —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∑–∞–∫–æ–Ω–∞–º –ö—ã—Ä–≥—ã–∑—Å–∫–æ–π –†–µ—Å–ø—É–±–ª–∏–∫–∏")
    print("üìñ –í–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n")
    
    db = get_index_db()
    
    while True:
        topic = input("‚ùì –í–∞—à —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if topic.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
            
        if not topic:
            print("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å\n")
            continue
            
        try:
            print("üîç –ü–æ–∏—Å–∫ –≤ –∑–∞–∫–æ–Ω–∞—Ö –ö–†...")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á—É–Ω–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            for k in [6, 4, 8]:
                message_content = get_message_content(topic, db, k)
                answer = get_model_response(topic, message_content)
                
                is_valid, validation_msg = validate_answer(answer, message_content)
                
                if is_valid:
                    break
                else:
                    logger.warning(f"–û—Ç–≤–µ—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é (k={k}): {validation_msg}")
            
            print(f"\nüìã –û—Ç–≤–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞:")
            print(f"{'='*50}")
            print(answer)
            print(f"{'='*50}\n")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            print("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.\n")

if __name__ == "__main__":
    # –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏–ª–∏ –æ–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    mode = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º (1 - –æ–¥–∏–Ω–æ—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å, 2 - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç): ").strip()
    
    if mode == "2":
        interactive_chat()
    else:
        db = get_index_db()
        NUMBER_RELEVANT_CHUNKS = 6  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        topic = input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: ")
        message_content = get_message_content(topic, db, NUMBER_RELEVANT_CHUNKS)
        answer = get_model_response(topic, message_content)
        print("\nüìã –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
        print(f"{'='*50}")
        print(answer)
        print(f"{'='*50}")